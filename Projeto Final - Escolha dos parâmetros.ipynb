{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Nanodegree Engenheiro de Machine Learning\n",
    "\n",
    "## Projeto Final\n",
    "\n",
    "### Fábio Corrêa Cordeiro\n",
    "### Agosto 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolhendo os parâmetros para melhorar o algoritmo\n",
    "\n",
    "Nesta última etapa iremos gerar novamente os vetores dos documentos e treinar o algoritmo de classificação alterando as principais escolhas que fizemos até então. O objetivo é identificar a combinação de hiperparâmetros e algoritimos de classificação que retorne a maior métrica F1. Os parâmetros que serão testados são:\n",
    "- Algoritmo de treinamento do Word2Vec: PV-DM ou PV-DBOW\n",
    "- Dimensão do vetor: 25, 50, 100, 200 ou 400\n",
    "- Janela de predição do algoritmo Word2Vec: 5, 10, 50, 100\n",
    "- Épocas de iteração do algoritmo Word2Vec: 5, 10 ou 20\n",
    "- Algoritmo de classificação: Decision Tree, Suport Vector Machine, Near Centroid, Gaussian Nayve Bayes, Random Forest e Neural Network.\n",
    "- Redução de dimensionalidade: 0%, 50%, 80%, 90% ou 95%\n",
    "\n",
    "Não será possível testar todas as combinações de parâmetro, seriam 3600 combinações diferentes. Portanto, para cada parâmetro, testaremos todas as opções mantendo o restante igual. Após encontrar os melhores valores para cada parâmentro faremos uma nova rodada testando cada parâmetros mantendo igual os demais. Após essas nova série de rodadas escolheremos a combinação com maior F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros iniciais\n",
    "\n",
    "Esse notebook é referente à primeira rodada e os parâmetros que serão mantidos constantes são:\n",
    "- Algoritmo de treinamento do Word2Vec: PV-DBOW\n",
    "- Dimensão do vetor: 100\n",
    "- Janela de predição do algoritmo Word2Vec: 50\n",
    "- Épocas de iteração do algoritmo Word2Vec: 10\n",
    "- Algoritmo de classificação: Decision Tree\n",
    "- Redução de dimensionalidade: 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from string import ascii_lowercase\n",
    "import unicodedata\n",
    "import gensim\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os arquivos preprocessados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total decorrido:  0:00:13.064448\n"
     ]
    }
   ],
   "source": [
    "momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "pastaentrada = \"corpus_preprocessado\" # Pasta onde estão os arquivos de entrada\n",
    "extensao = \"*.txt\"\n",
    "\n",
    "# Iterando os arquivos da pasta de entrada\n",
    "pastaArquivosCorpus = Path(pastaentrada).glob(extensao)\n",
    "\n",
    "docs = []\n",
    "arquivos = []\n",
    "\n",
    "# Iterando os arquivos da pasta atual\n",
    "for path in pastaArquivosCorpus:\n",
    "    path_arquivo = str(path)\n",
    "    texto = ''\n",
    "    with open(path_arquivo, 'r', encoding=\"UTF-8\") as f:\n",
    "        docs.append(gensim.utils.simple_preprocess(f.read()))\n",
    "        arquivos.append(os.path.basename(path_arquivo))\n",
    "\n",
    "momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo de Doc2Vec e algoritmos de classificação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de treinamento Word2Vec - PV-DM (dm = 1) ou **PV-DBOW (dm = 0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo de treinamento Word2Vec: dm = 1\n",
      "F1:  0.48761518771878\n",
      "Tempo total decorrido:  0:01:22.675008\n",
      " \n",
      "Algoritmo de treinamento Word2Vec: dm = 0\n",
      "F1:  0.5240175808146471\n",
      "Tempo total decorrido:  0:00:27.366233\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# dm ({1,0}, optional) – Defines the training algorithm.\n",
    "# If dm=1, ‘distributed memory’ (PV-DM) is used.\n",
    "# Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "param = [1, 0]\n",
    "\n",
    "for p in param:\n",
    "    print('Algoritmo de treinamento Word2Vec: dm =', p)\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "    # Lendo os documentos \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "    # Gerando modelo Doc2Vec\n",
    "    doc_model = Doc2Vec(documents, dm=p, vector_size=100, window=50, min_count=10, epochs=10, workers=3)\n",
    "\n",
    "    # Separando uma lista com os vetores de cada documento\n",
    "    docvectors = []\n",
    "\n",
    "    n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "    for n in range(n_vectors):\n",
    "        docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "    # Buscando os arquivos onde foi anotado o título e sub área\n",
    "    titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "    titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "    label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "    docvectors = np.array(docvectors)\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)  \n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        # treinando algoritmo decision tree e predizendo o conjunto de teste\n",
    "        clf_tree = tree.DecisionTreeClassifier()\n",
    "        clf_tree = clf_tree.fit(docvectors_train, label_train)\n",
    "        label_pred = clf_tree.predict(docvectors_test)\n",
    "\n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensão dos vetores: 25, **50**, 100, 200 ou 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos vetores:  25\n",
      "F1:  0.5727878219292022\n",
      "Tempo total decorrido:  0:01:16.672288\n",
      " \n",
      "Dimensão dos vetores:  50\n",
      "F1:  0.6129346547170632\n",
      "Tempo total decorrido:  0:01:18.177259\n",
      " \n",
      "Dimensão dos vetores:  100\n",
      "F1:  0.5000790564623241\n",
      "Tempo total decorrido:  0:01:27.411616\n",
      " \n",
      "Dimensão dos vetores:  200\n",
      "F1:  0.4922132003617016\n",
      "Tempo total decorrido:  0:01:40.341819\n",
      " \n",
      "Dimensão dos vetores:  400\n",
      "F1:  0.5612585412866473\n",
      "Tempo total decorrido:  0:02:43.696098\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Dimensão dos vetores\n",
    "param = [25, 50, 100, 200, 400]\n",
    "\n",
    "for p in param:\n",
    "    print('Dimensão dos vetores: ', p)\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "    # Lendo os documentos \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "    # Gerando modelo Doc2Vec\n",
    "    doc_model = Doc2Vec(documents, vector_size=p, window=50, min_count=10, epochs=10, workers=3)\n",
    "\n",
    "    # Separando uma lista com os vetores de cada documento\n",
    "    docvectors = []\n",
    "\n",
    "    n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "    for n in range(n_vectors):\n",
    "        docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "    # Buscando os arquivos onde foi anotado o título e sub área\n",
    "    titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "    titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "    label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "    docvectors = np.array(docvectors)\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        # treinando algoritmo decision tree e predizendo o conjunto de teste\n",
    "        clf_tree = tree.DecisionTreeClassifier()\n",
    "        clf_tree = clf_tree.fit(docvectors_train, label_train)\n",
    "        label_pred = clf_tree.predict(docvectors_test)\n",
    "\n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janela de predição do algoritmo Word2Vec: **5**, 10, 50 ou 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janela de predição:  5\n",
      "F1:  0.5892981307629017\n",
      "Tempo total decorrido:  0:00:38.604112\n",
      " \n",
      "Janela de predição:  10\n",
      "F1:  0.581948857505517\n",
      "Tempo total decorrido:  0:00:44.395981\n",
      " \n",
      "Janela de predição:  50\n",
      "F1:  0.5632723749915168\n",
      "Tempo total decorrido:  0:01:16.902453\n",
      " \n",
      "Janela de predição:  100\n",
      "F1:  0.5060360560070031\n",
      "Tempo total decorrido:  0:02:09.287733\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Janela de predição do algoritmo Word2Vec\n",
    "param = [5, 10, 50, 100]\n",
    "\n",
    "for p in param:\n",
    "    print('Janela de predição: ', p)\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "    # Lendo os documentos \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "    # Gerando modelo Doc2Vec\n",
    "    doc_model = Doc2Vec(documents, vector_size=100, window=p, min_count=10, epochs=10, workers=3)\n",
    "\n",
    "    # Separando uma lista com os vetores de cada documento\n",
    "    docvectors = []\n",
    "\n",
    "    n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "    for n in range(n_vectors):\n",
    "        docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "    # Buscando os arquivos onde foi anotado o título e sub área\n",
    "    titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "    titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "    label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "    docvectors = np.array(docvectors)\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        # treinando algoritmo decision tree e predizendo o conjunto de teste\n",
    "        clf_tree = tree.DecisionTreeClassifier()\n",
    "        clf_tree = clf_tree.fit(docvectors_train, label_train)\n",
    "        label_pred = clf_tree.predict(docvectors_test)\n",
    "\n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Épocas de iteração do algoritmo Word2Vec: 5, **10** ou 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épocas de iteração do algoritmo Word2Vec:  5\n",
      "F1:  0.5890648812254841\n",
      "Tempo total decorrido:  0:00:39.727839\n",
      " \n",
      "Épocas de iteração do algoritmo Word2Vec:  10\n",
      "F1:  0.6272032884103038\n",
      "Tempo total decorrido:  0:01:19.664315\n",
      " \n",
      "Épocas de iteração do algoritmo Word2Vec:  20\n",
      "F1:  0.46751212540940096\n",
      "Tempo total decorrido:  0:02:39.256106\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Épocas de iteração do algoritmo Word2Vec\n",
    "param = [5, 10, 20]\n",
    "\n",
    "for p in param:\n",
    "    print('Épocas de iteração do algoritmo Word2Vec: ', p)\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "    # Lendo os documentos \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "    # Gerando modelo Doc2Vec\n",
    "    doc_model = Doc2Vec(documents, vector_size=100, window=50, min_count=10, epochs=p, workers=3)\n",
    "\n",
    "    # Separando uma lista com os vetores de cada documento\n",
    "    docvectors = []\n",
    "\n",
    "    n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "    for n in range(n_vectors):\n",
    "        docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "    # Buscando os arquivos onde foi anotado o título e sub área\n",
    "    titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "    titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "    label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "    docvectors = np.array(docvectors)\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        # treinando algoritmo decision tree e predizendo o conjunto de teste\n",
    "        clf_tree = tree.DecisionTreeClassifier()\n",
    "        clf_tree = clf_tree.fit(docvectors_train, label_train)\n",
    "        label_pred = clf_tree.predict(docvectors_test)\n",
    "\n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de classificação: Decision Tree, Suport Vector Machine, **Near Centroid**, Gaussian Nayve Bayes, Random Forest e Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os documentos \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "# Gerando modelo Doc2Vec\n",
    "doc_model = Doc2Vec(documents, vector_size=100, window=50, min_count=10, epochs=10, workers=3)\n",
    "\n",
    "# Separando uma lista com os vetores de cada documento\n",
    "docvectors = []\n",
    "\n",
    "n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "for n in range(n_vectors):\n",
    "    docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "# Buscando os arquivos onde foi anotado o título e sub área\n",
    "titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "docvectors = np.array(docvectors)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo de classificação:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "F1:  0.5364317005498335\n",
      "Tempo total decorrido:  0:00:00.231939\n",
      " \n",
      "Algoritmo de classificação:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.5359554991774087\n",
      "Tempo total decorrido:  0:00:00.280157\n",
      " \n",
      "Algoritmo de classificação:  NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "F1:  0.7400664717161584\n",
      "Tempo total decorrido:  0:00:00.562891\n",
      " \n",
      "Algoritmo de classificação:  GaussianNB(priors=None)\n",
      "F1:  0.7222870520465816\n",
      "Tempo total decorrido:  0:00:00.135013\n",
      " \n",
      "Algoritmo de classificação:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "F1:  0.6099680390849772\n",
      "Tempo total decorrido:  0:00:00.470584\n",
      " \n",
      "Algoritmo de classificação:  MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "F1:  0.6632173656044313\n",
      "Tempo total decorrido:  0:00:02.037432\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Algoritmo de classificação\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "svc = svm.SVC(decision_function_shape='ovo')\n",
    "nc = NearestCentroid()\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "param = [dt, svc, nc, gnb, rf, nn]\n",
    "\n",
    "for clf in param:\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "    \n",
    "    print('Algoritmo de classificação: ', clf)\n",
    "    \n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        clf = clf.fit(docvectors_train, label_train)\n",
    "        label_pred = clf.predict(docvectors_test)\n",
    "\n",
    "        \n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redução de dimensionalidade: 0%, 50%, **80%**, 90% ou 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando algoritmo decision tree somente para usar a função 'feature_importances_'\n",
    "#clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_tree = clf_tree.fit(docvectors, label)\n",
    "\n",
    "\n",
    "def n_importante (n):\n",
    "    # Reduzindo os vetores de 100 dimensões para as n mais importantes \n",
    "    max_index = np.argsort(clf_tree.feature_importances_)[-n:]\n",
    "    max_value = np.sort(clf_tree.feature_importances_)[-n:]    # identificando 50 mais importante\n",
    "\n",
    "    docvectors_n_features = []\n",
    "    for vector in docvectors:\n",
    "        docvectors_n_features.append(vector[max_index])         # criando uma variável com os 50 mais importante\n",
    "\n",
    "    return (docvectors_n_features)\n",
    "\n",
    "docvectors                                   # sem redução\n",
    "docvectors_50features = n_importante (50)    # redução de 50%\n",
    "docvectors_20features = n_importante (20)    # redução de 80%\n",
    "docvectors_10features = n_importante (10)    # redução de 90%\n",
    "docvectors_5features = n_importante (5)      # redução de 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redução de dimensionalidade:  docvectors\n",
      "F1:  0.58513743572957\n",
      "Tempo total decorrido:  0:01:19.020753\n",
      " \n",
      "Redução de dimensionalidade:  docvectors_50features\n",
      "F1:  0.578799225778049\n",
      "Tempo total decorrido:  0:01:17.347825\n",
      " \n",
      "Redução de dimensionalidade:  docvectors_20features\n",
      "F1:  0.6216632167367037\n",
      "Tempo total decorrido:  0:01:18.376496\n",
      " \n",
      "Redução de dimensionalidade:  docvectors_10features\n",
      "F1:  0.6121784919427906\n",
      "Tempo total decorrido:  0:01:17.018657\n",
      " \n",
      "Redução de dimensionalidade:  docvectors_5features\n",
      "F1:  0.6104512343676007\n",
      "Tempo total decorrido:  0:01:17.780231\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Redução de dimensionalidade\n",
    "param = [[docvectors,'docvectors'],\n",
    "         [docvectors_50features, 'docvectors_50features'],\n",
    "         [docvectors_20features, 'docvectors_20features'],\n",
    "         [docvectors_10features, 'docvectors_10features'],\n",
    "         [docvectors_5features, 'docvectors_5features']]\n",
    "\n",
    "for p in param:\n",
    "    print('Redução de dimensionalidade: ', p[1])\n",
    "    \n",
    "    momentoInicial = datetime.datetime.now()   # Inicia um contador do tempo\n",
    "\n",
    "    # Lendo os documentos \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "    # Gerando modelo Doc2Vec\n",
    "    doc_model = Doc2Vec(documents, vector_size=100, window=50, min_count=10, epochs=10, workers=3)\n",
    "\n",
    "    # Separando uma lista com os vetores de cada documento\n",
    "    docvectors = []\n",
    "\n",
    "    n_vectors = len(range(len(doc_model.docvecs)))\n",
    "\n",
    "    for n in range(n_vectors):\n",
    "        docvectors.append(doc_model.docvecs[n])\n",
    "\n",
    "    # Buscando os arquivos onde foi anotado o título e sub área\n",
    "    titulo_label = pd.read_csv('lista_PRH_editada.csv')\n",
    "    titulo = titulo_label['titulo'].tolist()     # transformando a coluna \"titulo\" em lista\n",
    "    label = titulo_label['label'].tolist()       # transformando a coluna \"label\" em lista \n",
    "\n",
    "    docvectors = np.array(p[0])\n",
    "    label = np.array(label)\n",
    "\n",
    "    # Os dados serão divididos em 10 subconjuntos sendo 9 para treino e 1 para teste\n",
    "    # Serão feitos 10 treinamentos e testes e a métrica final é a média fe todos o F1 score\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in skf.split(docvectors, label):\n",
    "        docvectors_train, docvectors_test = docvectors[train_index], docvectors[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "\n",
    "        # treinando algoritmo decision tree e predizendo o conjunto de teste\n",
    "        #clf_tree = tree.DecisionTreeClassifier()\n",
    "        clf_tree = clf_tree.fit(docvectors_train, label_train)\n",
    "        label_pred = clf_tree.predict(docvectors_test)\n",
    "\n",
    "        # Definindo as métricas F1, Acurácia, Precisão e Revocação\n",
    "        precision_n, recall_n, f1_n, support = precision_recall_fscore_support(label_test, label_pred, average='weighted')\n",
    "        precision.append(precision_n)\n",
    "        recall.append(recall_n)\n",
    "        f1.append(f1_n)\n",
    "        acc.append(accuracy_score(label_test, label_pred))\n",
    "\n",
    "    print ('F1: ', np.mean(f1))\n",
    "\n",
    "    momentoFinal = datetime.datetime.now() #Encerrando o contador do tempo\n",
    "    print(\"Tempo total decorrido: \", momentoFinal - momentoInicial)\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
